# Chapter 3: Probability and Information Theory

**Probability theory** is a mathematical framework for representing uncertain statements. <br>
AI applications uses probability theory in many ways like Chatbot that determine the most likely next tokens by selecting the tokens with the highest probability in a range of others. 

**Information theory** allows us to quantify the amount of uncertainly in a probabilty distribution.

## 3.1 Why probability ?

**Machine learning** always deals with uncertain quantities and stochastic quantities. Probability can help us quantifying uncertainty. <br>
Here are the three possible sources of uncertainty:
There are three possible sources of uncertainty:
1. Inherent stochasticity in the system being modeled. For example in
quantum mechanics and hypothetical card game
2. Incomplete observability. Even deterministic systems can appear stochastic
when we cannot observe all of the variables that drive the behavior of the
system.
3. Incomplete modeling. When we use a model that must discard some of
the information we have observed, the discarded information results in
uncertainty in the modelâ€™s predictions. <br>
**Probability theory** provides a set of formal rules for determining the
likelihood of a proposition being true given the likelihood of other propositions.

## 3.2 Random Variables

A **random variable** is a variable that can take on different values randomly. To represent the random variable of x, we can write x1 or x2. x1 or x2 describes the possible state of the variable x.<br>
*Random variables* may be discrete or continuous. A discrete random variable is one that has a finite or countably infinite number of states.

## 3.3 Probability Distributions

### 3.3.1 Discrete Variables and Probability Mass Functions