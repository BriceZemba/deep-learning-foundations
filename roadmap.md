**Deep Learning â€“ Goodfellow, Bengio, Courville**.


## 1ï¸âƒ£ Repository structure

```text
deep-learning-goodfellow/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ roadmap.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ notes/
â”‚   â”œâ”€â”€ ch01_introduction.md
â”‚   â”œâ”€â”€ ch02_linear_algebra.md
â”‚   â”œâ”€â”€ ch03_probability.md
â”‚   â”œâ”€â”€ ch04_numerical_computation.md
â”‚   â”œâ”€â”€ ch05_machine_learning_basics.md
â”‚   â”œâ”€â”€ ch06_deep_feedforward_networks.md
â”‚   â”œâ”€â”€ ch07_regularization.md
â”‚   â”œâ”€â”€ ch08_optimization.md
â”‚   â”œâ”€â”€ ch09_convnets.md
â”‚   â”œâ”€â”€ ch10_rnns.md
â”‚   â”œâ”€â”€ ch11_practical_methodology.md
â”‚   â”œâ”€â”€ ch12_applications.md
â”‚   â””â”€â”€ ch13_generative_models.md
â”‚
â”œâ”€â”€ math/
â”‚   â”œâ”€â”€ linear_algebra.md
â”‚   â”œâ”€â”€ probability_theory.md
â”‚   â”œâ”€â”€ optimization.md
â”‚   â””â”€â”€ information_theory.md
â”‚
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ fundamentals/
â”‚   â”‚   â”œâ”€â”€ backprop_from_scratch.py
â”‚   â”‚   â”œâ”€â”€ gradient_descent.py
â”‚   â”‚   â””â”€â”€ logistic_regression.py
â”‚   â”‚
â”‚   â”œâ”€â”€ neural_networks/
â”‚   â”‚   â”œâ”€â”€ mlp_pytorch.py
â”‚   â”‚   â”œâ”€â”€ regularization.py
â”‚   â”‚   â””â”€â”€ batch_norm.py
â”‚   â”‚
â”‚   â”œâ”€â”€ convnets/
â”‚   â”‚   â”œâ”€â”€ cnn_mnist.py
â”‚   â”‚   â””â”€â”€ cnn_cifar10.py
â”‚   â”‚
â”‚   â”œâ”€â”€ rnns/
â”‚   â”‚   â”œâ”€â”€ rnn_from_scratch.py
â”‚   â”‚   â””â”€â”€ lstm_pytorch.py
â”‚   â”‚
â”‚   â””â”€â”€ generative_models/
â”‚       â”œâ”€â”€ autoencoder.py
â”‚       â””â”€â”€ vae.py
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ optimization_study.md
â”‚   â”œâ”€â”€ regularization_comparison.md
â”‚   â””â”€â”€ architecture_ablation.md
â”‚
â””â”€â”€ references/
    â”œâ”€â”€ papers.md
    â”œâ”€â”€ lecture_notes.md
    â””â”€â”€ external_resources.md
```

---

## Repository Structure
- `notes/` â€“ Chapter summaries and explanations
- `math/` â€“ Mathematical prerequisites and derivations
- `code/` â€“ Implementations (from scratch + PyTorch)
- `experiments/` â€“ Empirical studies and ablation experiments

## Progress
- [ ] Chapter 1 â€“ Introduction
- [ ] Chapter 2 â€“ Linear Algebra
- [ ] Chapter 3 â€“ Probability and Information Theory
- [ ] Chapter 4 â€“ Numerical Computation
- [ ] Chapter 5 â€“ Machine Learning Basics
- [ ] Chapter 6 â€“ Deep Feedforward Networks
- [ ] Chapter 7 â€“ Regularization
- [ ] Chapter 8 â€“ Optimization
- [ ] Chapter 9 â€“ Convolutional Networks
- [ ] Chapter 10 â€“ Recurrent Networks
- [ ] Chapter 11 â€“ Practical Methodology
- [ ] Chapter 12 â€“ Applications
- [ ] Chapter 13 â€“ Linear Factor Models
- [ ] Chapter 14â€“20 â€“ Advanced Topics (Selective)

```

---


### Mathematical foundations

* Ch. 1: Overview of Deep Learning
* Ch. 2: Linear Algebra â†’ `math/linear_algebra.md`
* Ch. 3: Probability & Information Theory
* Ch. 4: Numerical Computation

Goal: full mathematical readiness

---

### Core ML concepts

* Ch. 5: Machine Learning Basics
* Implement:

  * Logistic regression
  * Gradient descent
  * Loss functions

ğŸ“ `code/fundamentals/`

---

### Deep feedforward networks

* Ch. 6: Deep Feedforward Networks
* Ch. 7: Regularization
* Implement:

  * MLP from scratch
  * Dropout, L2, batch normalization

ğŸ“ `neural_networks/`

---

### Optimization

* Ch. 8: Optimization for Deep Learning
* Experiments:

  * SGD vs Adam vs RMSProp
  * Learning rate schedules

ğŸ“ `experiments/optimization_study.md`

---

### Specialized architectures

* Ch. 9: Convolutional Networks
* Ch. 10: Recurrent Networks
* Implement:

  * CNN on CIFAR-10
  * RNN / LSTM sequence modeling

ğŸ“ `convnets/`, `rnns/`

---

### Generative models & methodology

* Ch. 11: Practical Methodology
* Ch. 13â€“14: Autoencoders, VAEs
* Final experiments + documentation

ğŸ“ `generative_models/`

---
